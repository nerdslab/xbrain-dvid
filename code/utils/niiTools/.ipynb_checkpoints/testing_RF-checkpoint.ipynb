{
 "metadata": {
  "name": "",
  "signature": "sha256:5602f1ee6874a8aa820276b6241f3872801a636677cb2908d095e1c56dee2b0d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Description of the process"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "dsadas"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import preprocessing ,feature_extraction\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "#from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "#from sklearn.ensemble import ExtraTreesClassifier\n",
      "#from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "from sklearn import preprocessing ,feature_extraction\n",
      "\n",
      "from datetime import datetime\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn.metrics import log_loss\n",
      "from sklearn.cross_validation import StratifiedKFold\n",
      "\n",
      "from sklearn.metrics import confusion_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def RF(Xr, Yr, Xt):\n",
      "    rf = RandomForestClassifier(n_estimators=50) #n_jobs=-1,bootstrap=False, min_samples_leaf=3,n_estimators=200, min_samples_split=4, criterion='entropy', max_features=27, max_depth=None)\n",
      "    #print 'fitting RF'\n",
      "    rf.fit(Xr,Yr)\n",
      "    #print 'Getting predictions'\n",
      "    Yt = rf.predict_proba(Xt)\n",
      "    #print Yt\n",
      "    FI = rf.feature_importances_\n",
      "    return Yt,FI\n",
      "\n",
      "def GBC(Xr, Yr, Xt):\n",
      "    #lr = GradientBoostingClassifier(n_estimators=500,max_depth=5)\n",
      "    lr = GradientBoostingClassifier(n_estimators=30)\n",
      "    #print 'fitting GBC'\n",
      "    lr.fit(Xr,Yr)\n",
      "    #print 'Getting predictions'\n",
      "    Yt = lr.predict_proba(Xt)\n",
      "    FI = lr.feature_importances_\n",
      "    return Yt, FI\n",
      "\n",
      "def KNN(Xr, Yr, Xt):\n",
      "    nn = KNeighborsClassifier()\n",
      "    print 'fitting NN'\n",
      "    nn.fit(Xr,Yr)\n",
      "    print 'Getting predictions'\n",
      "    Yt = nn.predict_proba(Xt)\n",
      "    FI = []\n",
      "    return Yt, FI"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 280
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def confmat(conf_a):\n",
      "    norm_conf = []\n",
      "    for i in conf_a:\n",
      "        a = 0\n",
      "        tmp_arr = []\n",
      "        a = sum(i, 0)\n",
      "        for j in i:\n",
      "            tmp_arr.append(float(j)/float(a))\n",
      "        norm_conf.append(tmp_arr)\n",
      "\n",
      "    fig = plt.figure()\n",
      "    plt.clf()\n",
      "    ax = fig.add_subplot(111)\n",
      "    ax.set_aspect(1)\n",
      "    res = ax.imshow(np.array(norm_conf), cmap=plt.cm.jet, \n",
      "                interpolation='nearest')\n",
      "\n",
      "    width = len(conf_a)\n",
      "    height = len(conf_a[0])\n",
      "\n",
      "    for x in xrange(width):\n",
      "        for y in xrange(height):\n",
      "            ax.annotate(str(conf_a[x][y]), xy=(y, x), \n",
      "                    horizontalalignment='center',\n",
      "                    verticalalignment='center')\n",
      "\n",
      "    cb = fig.colorbar(res)\n",
      "    alphabet = '0123456789'\n",
      "    plt.xticks(range(width), alphabet[:width])\n",
      "    plt.yticks(range(height), alphabet[:height])\n",
      "    plt.show() #savefig('confusion_matrix.png', format='png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 250
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datafiles = {}\n",
      "datafiles['train'] = 'test_4_13_featurematrix.csv'\n",
      "#datafiles['train_label'] = 'test_4_10_labels.csv'\n",
      "datafiles['test'] = 'test_4_13_testset_featurematrix.csv'\n",
      "start = datetime.now()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 251
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Loading features'\n",
      "    \n",
      "# import data\n",
      "traind = pd.read_csv(datafiles['train'])\n",
      "    \n",
      "# drop ids and get labels\n",
      "labels = traind.Labels.values\n",
      "#X_train = traind.drop('id', axis=1)\n",
      "X_train = traind.drop('Unnamed: 18', axis=1)\n",
      "X_train = X_train.drop('Mean_raw', axis=1)\n",
      "X_train = X_train.drop('Labels', axis=1)\n",
      "\n",
      "#labels = pd.read_csv(datafiles['train_label'], index_col=None, names=None)\n",
      "# encode labels \n",
      "lbl_enc = preprocessing.LabelEncoder()\n",
      "Y_train = lbl_enc.fit_transform(labels)\n",
      "\n",
      "#feature_names=np.array(X_train.columns.tolist()) \n",
      "testd = pd.read_csv(datafiles['test'])\n",
      "labels_test = testd.Labels.values\n",
      "#X_train = traind.drop('id', axis=1)\n",
      "X_test = testd.drop('Unnamed: 18', axis=1)\n",
      "X_test = X_test.drop('Mean_raw', axis=1)\n",
      "X_test = X_test.drop('Labels', axis=1)\n",
      "\n",
      "Y_test = lbl_enc.fit_transform(labels_test)\n",
      "\n",
      "X_test = preprocessing.scale(X_test.values)    \n",
      "X_train = preprocessing.scale(X_train.values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading features\n"
       ]
      }
     ],
     "prompt_number": 252
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 206
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aval = {}\n",
      "aval['method'] = GBC\n",
      "print 'CV!'\n",
      "n_cv = 10\n",
      "cost_fun_cv = []\n",
      "skf = StratifiedKFold(Y_train, n_cv, shuffle=True, random_state=1)  #H#\n",
      "i=1\n",
      "\n",
      "n_classes = np.size(np.unique(Y_train))      #H#\n",
      "Y_pred_Tot=np.zeros((len(Y_train), n_classes))    #H#\n",
      "\n",
      "for train, test in skf:\n",
      "    #print '...runnning cv-fold', i, 'of', n_cv\n",
      "    i+=1\n",
      "    idx_t = test\n",
      "    idx_r = train\n",
      "    Xr = X_train[idx_r,:]\n",
      " \n",
      "    Yr = Y_train[idx_r]               \n",
      "    Xt = X_train[idx_t,:]\n",
      "                        \n",
      "    Yt_true = Y_train[idx_t]\n",
      "    Yt_pred, FI = aval['method'](Xr, Yr , Xt)\n",
      "    #print FI\n",
      "\n",
      "    Y_pred_Tot[idx_t,:]= Yt_pred  \n",
      "\n",
      "    cost_fun =log_loss(Yt_true, Yt_pred) \n",
      "    #print 'cost_fun',cost_fun\n",
      "    cost_fun_cv.append(cost_fun)\n",
      "  \n",
      "print(\"cost_fun: %0.4f (+/- %0.4f)\" % (np.mean(cost_fun_cv), np.std(cost_fun_cv)/np.sqrt(n_cv)))\n",
      "\n",
      "\n",
      "conf_arr=confusion_matrix(Y_train, np.argmax(Y_pred_Tot,axis=1))\n",
      "confmat(conf_arr)   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CV!\n",
        "cost_fun: 0.2179 (+/- 0.0087)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAD7CAYAAAAsJIKcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niUhSlJREJCn/H+XXnDxpff+dAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x110a10110>"
       ]
      }
     ],
     "prompt_number": 389
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aval = {}\n",
      "aval['method'] = GBC\n",
      "Yt_pred,FI = aval['method'](X_train, Y_train, X_test)\n",
      "Yt_pred = preprocessing.normalize(Yt_pred, norm='l1', axis=1)       \n",
      "cost_fun = log_loss(Y_test, Yt_pred) \n",
      "print(\"cost_fun: %0.4f \" % (np.mean(cost_fun)))\n",
      "conf_arr_test = confusion_matrix(Y_test, np.argmax(Yt_pred,axis=1))\n",
      "confmat(conf_arr_test)      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cost_fun: 2.3182 \n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAEACAYAAAD1BmDyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x110722e90>"
       ]
      }
     ],
     "prompt_number": 390
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample = pd.read_csv(datafiles['test'])          \n",
      "    \n",
      "#Yt_pred,FI = aval['method'](X_train, Y_train, X_test)\n",
      "Yt_pred_proc = preprocessing.normalize(Yt_pred, norm='l1', axis=1)       \n",
      "\n",
      "aux = np.transpose(np.atleast_2d(np.argmax(Yt_pred_proc,axis=1)+1))\n",
      "File = np.concatenate((Yt_pred, aux), axis=1)                       \n",
      "File = pd.DataFrame(File,  columns=['Nucleus','Vessels','Cell Body','Progress','Background','Predicted'])\n",
      "File.to_csv('test_4_13_testset_predicions.csv', index_label='id')\n",
      "File"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Nucleus</th>\n",
        "      <th>Vessels</th>\n",
        "      <th>Cell Body</th>\n",
        "      <th>Progress</th>\n",
        "      <th>Background</th>\n",
        "      <th>Predicted</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0  </th>\n",
        "      <td> 0.004346</td>\n",
        "      <td> 0.907481</td>\n",
        "      <td> 0.081508</td>\n",
        "      <td> 0.002947</td>\n",
        "      <td> 0.003718</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1  </th>\n",
        "      <td> 0.004346</td>\n",
        "      <td> 0.907481</td>\n",
        "      <td> 0.081508</td>\n",
        "      <td> 0.002947</td>\n",
        "      <td> 0.003718</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2  </th>\n",
        "      <td> 0.004588</td>\n",
        "      <td> 0.911348</td>\n",
        "      <td> 0.077709</td>\n",
        "      <td> 0.002810</td>\n",
        "      <td> 0.003545</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3  </th>\n",
        "      <td> 0.002495</td>\n",
        "      <td> 0.942222</td>\n",
        "      <td> 0.050872</td>\n",
        "      <td> 0.001951</td>\n",
        "      <td> 0.002461</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4  </th>\n",
        "      <td> 0.004145</td>\n",
        "      <td> 0.911753</td>\n",
        "      <td> 0.077744</td>\n",
        "      <td> 0.002811</td>\n",
        "      <td> 0.003546</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5  </th>\n",
        "      <td> 0.004976</td>\n",
        "      <td> 0.894067</td>\n",
        "      <td> 0.093325</td>\n",
        "      <td> 0.003375</td>\n",
        "      <td> 0.004257</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6  </th>\n",
        "      <td> 0.004374</td>\n",
        "      <td> 0.894608</td>\n",
        "      <td> 0.093381</td>\n",
        "      <td> 0.003377</td>\n",
        "      <td> 0.004260</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7  </th>\n",
        "      <td> 0.297120</td>\n",
        "      <td> 0.055313</td>\n",
        "      <td> 0.583388</td>\n",
        "      <td> 0.026746</td>\n",
        "      <td> 0.037433</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8  </th>\n",
        "      <td> 0.321268</td>\n",
        "      <td> 0.053399</td>\n",
        "      <td> 0.563202</td>\n",
        "      <td> 0.025821</td>\n",
        "      <td> 0.036311</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9  </th>\n",
        "      <td> 0.285289</td>\n",
        "      <td> 0.060853</td>\n",
        "      <td> 0.592235</td>\n",
        "      <td> 0.025681</td>\n",
        "      <td> 0.035942</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10 </th>\n",
        "      <td> 0.308901</td>\n",
        "      <td> 0.058828</td>\n",
        "      <td> 0.572531</td>\n",
        "      <td> 0.024827</td>\n",
        "      <td> 0.034913</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11 </th>\n",
        "      <td> 0.312638</td>\n",
        "      <td> 0.077058</td>\n",
        "      <td> 0.532269</td>\n",
        "      <td> 0.032520</td>\n",
        "      <td> 0.045514</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12 </th>\n",
        "      <td> 0.340128</td>\n",
        "      <td> 0.067159</td>\n",
        "      <td> 0.517014</td>\n",
        "      <td> 0.031588</td>\n",
        "      <td> 0.044111</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13 </th>\n",
        "      <td> 0.268693</td>\n",
        "      <td> 0.056367</td>\n",
        "      <td> 0.611404</td>\n",
        "      <td> 0.026512</td>\n",
        "      <td> 0.037023</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14 </th>\n",
        "      <td> 0.189977</td>\n",
        "      <td> 0.082441</td>\n",
        "      <td> 0.634659</td>\n",
        "      <td> 0.038776</td>\n",
        "      <td> 0.054148</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15 </th>\n",
        "      <td> 0.189977</td>\n",
        "      <td> 0.082441</td>\n",
        "      <td> 0.634659</td>\n",
        "      <td> 0.038776</td>\n",
        "      <td> 0.054148</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16 </th>\n",
        "      <td> 0.081083</td>\n",
        "      <td> 0.055575</td>\n",
        "      <td> 0.802391</td>\n",
        "      <td> 0.024448</td>\n",
        "      <td> 0.036503</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17 </th>\n",
        "      <td> 0.077146</td>\n",
        "      <td> 0.061982</td>\n",
        "      <td> 0.811796</td>\n",
        "      <td> 0.023141</td>\n",
        "      <td> 0.025935</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18 </th>\n",
        "      <td> 0.050456</td>\n",
        "      <td> 0.181243</td>\n",
        "      <td> 0.636383</td>\n",
        "      <td> 0.053748</td>\n",
        "      <td> 0.078170</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19 </th>\n",
        "      <td> 0.014258</td>\n",
        "      <td> 0.939852</td>\n",
        "      <td> 0.023000</td>\n",
        "      <td> 0.009435</td>\n",
        "      <td> 0.013455</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20 </th>\n",
        "      <td> 0.012211</td>\n",
        "      <td> 0.942240</td>\n",
        "      <td> 0.022672</td>\n",
        "      <td> 0.008080</td>\n",
        "      <td> 0.014796</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21 </th>\n",
        "      <td> 0.012188</td>\n",
        "      <td> 0.940402</td>\n",
        "      <td> 0.022628</td>\n",
        "      <td> 0.008064</td>\n",
        "      <td> 0.016718</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22 </th>\n",
        "      <td> 0.014193</td>\n",
        "      <td> 0.935567</td>\n",
        "      <td> 0.024181</td>\n",
        "      <td> 0.009392</td>\n",
        "      <td> 0.016667</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23 </th>\n",
        "      <td> 0.029351</td>\n",
        "      <td> 0.860960</td>\n",
        "      <td> 0.050005</td>\n",
        "      <td> 0.019422</td>\n",
        "      <td> 0.040263</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24 </th>\n",
        "      <td> 0.014215</td>\n",
        "      <td> 0.936989</td>\n",
        "      <td> 0.025976</td>\n",
        "      <td> 0.009406</td>\n",
        "      <td> 0.013414</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25 </th>\n",
        "      <td> 0.003938</td>\n",
        "      <td> 0.935750</td>\n",
        "      <td> 0.053245</td>\n",
        "      <td> 0.002704</td>\n",
        "      <td> 0.004363</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26 </th>\n",
        "      <td> 0.003865</td>\n",
        "      <td> 0.961657</td>\n",
        "      <td> 0.029973</td>\n",
        "      <td> 0.001979</td>\n",
        "      <td> 0.002526</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27 </th>\n",
        "      <td> 0.004225</td>\n",
        "      <td> 0.944687</td>\n",
        "      <td> 0.042514</td>\n",
        "      <td> 0.003894</td>\n",
        "      <td> 0.004680</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28 </th>\n",
        "      <td> 0.005226</td>\n",
        "      <td> 0.936991</td>\n",
        "      <td> 0.031968</td>\n",
        "      <td> 0.022512</td>\n",
        "      <td> 0.003302</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29 </th>\n",
        "      <td> 0.060169</td>\n",
        "      <td> 0.905453</td>\n",
        "      <td> 0.028043</td>\n",
        "      <td> 0.002835</td>\n",
        "      <td> 0.003500</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>143</th>\n",
        "      <td> 0.130391</td>\n",
        "      <td> 0.623509</td>\n",
        "      <td> 0.148826</td>\n",
        "      <td> 0.078306</td>\n",
        "      <td> 0.018968</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>144</th>\n",
        "      <td> 0.126296</td>\n",
        "      <td> 0.715064</td>\n",
        "      <td> 0.018934</td>\n",
        "      <td> 0.121733</td>\n",
        "      <td> 0.017972</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>145</th>\n",
        "      <td> 0.063437</td>\n",
        "      <td> 0.695232</td>\n",
        "      <td> 0.086819</td>\n",
        "      <td> 0.136674</td>\n",
        "      <td> 0.017838</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>146</th>\n",
        "      <td> 0.078073</td>\n",
        "      <td> 0.829263</td>\n",
        "      <td> 0.010649</td>\n",
        "      <td> 0.072547</td>\n",
        "      <td> 0.009468</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>147</th>\n",
        "      <td> 0.063906</td>\n",
        "      <td> 0.831177</td>\n",
        "      <td> 0.022712</td>\n",
        "      <td> 0.072714</td>\n",
        "      <td> 0.009490</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>148</th>\n",
        "      <td> 0.056226</td>\n",
        "      <td> 0.834623</td>\n",
        "      <td> 0.050699</td>\n",
        "      <td> 0.046378</td>\n",
        "      <td> 0.012073</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>149</th>\n",
        "      <td> 0.133610</td>\n",
        "      <td> 0.724981</td>\n",
        "      <td> 0.031588</td>\n",
        "      <td> 0.095438</td>\n",
        "      <td> 0.014384</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>150</th>\n",
        "      <td> 0.119589</td>\n",
        "      <td> 0.670357</td>\n",
        "      <td> 0.098951</td>\n",
        "      <td> 0.085423</td>\n",
        "      <td> 0.025679</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>151</th>\n",
        "      <td> 0.067259</td>\n",
        "      <td> 0.847214</td>\n",
        "      <td> 0.015901</td>\n",
        "      <td> 0.055479</td>\n",
        "      <td> 0.014147</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>152</th>\n",
        "      <td> 0.118028</td>\n",
        "      <td> 0.661611</td>\n",
        "      <td> 0.097660</td>\n",
        "      <td> 0.097356</td>\n",
        "      <td> 0.025344</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>153</th>\n",
        "      <td> 0.067259</td>\n",
        "      <td> 0.847214</td>\n",
        "      <td> 0.015901</td>\n",
        "      <td> 0.055479</td>\n",
        "      <td> 0.014147</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>154</th>\n",
        "      <td> 0.133457</td>\n",
        "      <td> 0.748096</td>\n",
        "      <td> 0.035652</td>\n",
        "      <td> 0.058102</td>\n",
        "      <td> 0.024693</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>155</th>\n",
        "      <td> 0.284247</td>\n",
        "      <td> 0.348268</td>\n",
        "      <td> 0.073233</td>\n",
        "      <td> 0.234463</td>\n",
        "      <td> 0.059789</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>156</th>\n",
        "      <td> 0.035543</td>\n",
        "      <td> 0.713970</td>\n",
        "      <td> 0.173487</td>\n",
        "      <td> 0.038218</td>\n",
        "      <td> 0.038781</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>157</th>\n",
        "      <td> 0.219529</td>\n",
        "      <td> 0.360785</td>\n",
        "      <td> 0.210515</td>\n",
        "      <td> 0.091668</td>\n",
        "      <td> 0.117503</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>158</th>\n",
        "      <td> 0.149985</td>\n",
        "      <td> 0.266446</td>\n",
        "      <td> 0.149941</td>\n",
        "      <td> 0.091946</td>\n",
        "      <td> 0.341683</td>\n",
        "      <td> 5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>159</th>\n",
        "      <td> 0.368269</td>\n",
        "      <td> 0.234185</td>\n",
        "      <td> 0.134925</td>\n",
        "      <td> 0.080813</td>\n",
        "      <td> 0.181808</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>160</th>\n",
        "      <td> 0.400207</td>\n",
        "      <td> 0.327194</td>\n",
        "      <td> 0.094070</td>\n",
        "      <td> 0.046876</td>\n",
        "      <td> 0.131654</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>161</th>\n",
        "      <td> 0.429004</td>\n",
        "      <td> 0.176616</td>\n",
        "      <td> 0.208016</td>\n",
        "      <td> 0.146614</td>\n",
        "      <td> 0.039750</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>162</th>\n",
        "      <td> 0.474871</td>\n",
        "      <td> 0.068722</td>\n",
        "      <td> 0.163087</td>\n",
        "      <td> 0.254407</td>\n",
        "      <td> 0.038913</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>163</th>\n",
        "      <td> 0.171240</td>\n",
        "      <td> 0.181127</td>\n",
        "      <td> 0.456363</td>\n",
        "      <td> 0.074865</td>\n",
        "      <td> 0.116405</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>164</th>\n",
        "      <td> 0.160967</td>\n",
        "      <td> 0.184973</td>\n",
        "      <td> 0.441454</td>\n",
        "      <td> 0.088733</td>\n",
        "      <td> 0.123874</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>165</th>\n",
        "      <td> 0.199225</td>\n",
        "      <td> 0.170550</td>\n",
        "      <td> 0.508756</td>\n",
        "      <td> 0.056340</td>\n",
        "      <td> 0.065129</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>166</th>\n",
        "      <td> 0.165428</td>\n",
        "      <td> 0.166139</td>\n",
        "      <td> 0.567571</td>\n",
        "      <td> 0.046782</td>\n",
        "      <td> 0.054080</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>167</th>\n",
        "      <td> 0.131515</td>\n",
        "      <td> 0.177146</td>\n",
        "      <td> 0.528747</td>\n",
        "      <td> 0.102218</td>\n",
        "      <td> 0.060374</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>168</th>\n",
        "      <td> 0.033130</td>\n",
        "      <td> 0.627634</td>\n",
        "      <td> 0.156197</td>\n",
        "      <td> 0.168343</td>\n",
        "      <td> 0.014696</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>169</th>\n",
        "      <td> 0.034706</td>\n",
        "      <td> 0.412148</td>\n",
        "      <td> 0.226977</td>\n",
        "      <td> 0.304815</td>\n",
        "      <td> 0.021355</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>170</th>\n",
        "      <td> 0.023000</td>\n",
        "      <td> 0.644012</td>\n",
        "      <td> 0.102672</td>\n",
        "      <td> 0.215236</td>\n",
        "      <td> 0.015079</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>171</th>\n",
        "      <td> 0.022139</td>\n",
        "      <td> 0.280128</td>\n",
        "      <td> 0.099459</td>\n",
        "      <td> 0.583759</td>\n",
        "      <td> 0.014515</td>\n",
        "      <td> 4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>172</th>\n",
        "      <td> 0.019158</td>\n",
        "      <td> 0.296536</td>\n",
        "      <td> 0.088398</td>\n",
        "      <td> 0.583346</td>\n",
        "      <td> 0.012560</td>\n",
        "      <td> 4</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>173 rows \u00d7 6 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 437,
       "text": [
        "      Nucleus   Vessels  Cell Body  Progress  Background  Predicted\n",
        "0    0.004346  0.907481   0.081508  0.002947    0.003718          2\n",
        "1    0.004346  0.907481   0.081508  0.002947    0.003718          2\n",
        "2    0.004588  0.911348   0.077709  0.002810    0.003545          2\n",
        "3    0.002495  0.942222   0.050872  0.001951    0.002461          2\n",
        "4    0.004145  0.911753   0.077744  0.002811    0.003546          2\n",
        "5    0.004976  0.894067   0.093325  0.003375    0.004257          2\n",
        "6    0.004374  0.894608   0.093381  0.003377    0.004260          2\n",
        "7    0.297120  0.055313   0.583388  0.026746    0.037433          3\n",
        "8    0.321268  0.053399   0.563202  0.025821    0.036311          3\n",
        "9    0.285289  0.060853   0.592235  0.025681    0.035942          3\n",
        "10   0.308901  0.058828   0.572531  0.024827    0.034913          3\n",
        "11   0.312638  0.077058   0.532269  0.032520    0.045514          3\n",
        "12   0.340128  0.067159   0.517014  0.031588    0.044111          3\n",
        "13   0.268693  0.056367   0.611404  0.026512    0.037023          3\n",
        "14   0.189977  0.082441   0.634659  0.038776    0.054148          3\n",
        "15   0.189977  0.082441   0.634659  0.038776    0.054148          3\n",
        "16   0.081083  0.055575   0.802391  0.024448    0.036503          3\n",
        "17   0.077146  0.061982   0.811796  0.023141    0.025935          3\n",
        "18   0.050456  0.181243   0.636383  0.053748    0.078170          3\n",
        "19   0.014258  0.939852   0.023000  0.009435    0.013455          2\n",
        "20   0.012211  0.942240   0.022672  0.008080    0.014796          2\n",
        "21   0.012188  0.940402   0.022628  0.008064    0.016718          2\n",
        "22   0.014193  0.935567   0.024181  0.009392    0.016667          2\n",
        "23   0.029351  0.860960   0.050005  0.019422    0.040263          2\n",
        "24   0.014215  0.936989   0.025976  0.009406    0.013414          2\n",
        "25   0.003938  0.935750   0.053245  0.002704    0.004363          2\n",
        "26   0.003865  0.961657   0.029973  0.001979    0.002526          2\n",
        "27   0.004225  0.944687   0.042514  0.003894    0.004680          2\n",
        "28   0.005226  0.936991   0.031968  0.022512    0.003302          2\n",
        "29   0.060169  0.905453   0.028043  0.002835    0.003500          2\n",
        "..        ...       ...        ...       ...         ...        ...\n",
        "143  0.130391  0.623509   0.148826  0.078306    0.018968          2\n",
        "144  0.126296  0.715064   0.018934  0.121733    0.017972          2\n",
        "145  0.063437  0.695232   0.086819  0.136674    0.017838          2\n",
        "146  0.078073  0.829263   0.010649  0.072547    0.009468          2\n",
        "147  0.063906  0.831177   0.022712  0.072714    0.009490          2\n",
        "148  0.056226  0.834623   0.050699  0.046378    0.012073          2\n",
        "149  0.133610  0.724981   0.031588  0.095438    0.014384          2\n",
        "150  0.119589  0.670357   0.098951  0.085423    0.025679          2\n",
        "151  0.067259  0.847214   0.015901  0.055479    0.014147          2\n",
        "152  0.118028  0.661611   0.097660  0.097356    0.025344          2\n",
        "153  0.067259  0.847214   0.015901  0.055479    0.014147          2\n",
        "154  0.133457  0.748096   0.035652  0.058102    0.024693          2\n",
        "155  0.284247  0.348268   0.073233  0.234463    0.059789          2\n",
        "156  0.035543  0.713970   0.173487  0.038218    0.038781          2\n",
        "157  0.219529  0.360785   0.210515  0.091668    0.117503          2\n",
        "158  0.149985  0.266446   0.149941  0.091946    0.341683          5\n",
        "159  0.368269  0.234185   0.134925  0.080813    0.181808          1\n",
        "160  0.400207  0.327194   0.094070  0.046876    0.131654          1\n",
        "161  0.429004  0.176616   0.208016  0.146614    0.039750          1\n",
        "162  0.474871  0.068722   0.163087  0.254407    0.038913          1\n",
        "163  0.171240  0.181127   0.456363  0.074865    0.116405          3\n",
        "164  0.160967  0.184973   0.441454  0.088733    0.123874          3\n",
        "165  0.199225  0.170550   0.508756  0.056340    0.065129          3\n",
        "166  0.165428  0.166139   0.567571  0.046782    0.054080          3\n",
        "167  0.131515  0.177146   0.528747  0.102218    0.060374          3\n",
        "168  0.033130  0.627634   0.156197  0.168343    0.014696          2\n",
        "169  0.034706  0.412148   0.226977  0.304815    0.021355          2\n",
        "170  0.023000  0.644012   0.102672  0.215236    0.015079          2\n",
        "171  0.022139  0.280128   0.099459  0.583759    0.014515          4\n",
        "172  0.019158  0.296536   0.088398  0.583346    0.012560          4\n",
        "\n",
        "[173 rows x 6 columns]"
       ]
      }
     ],
     "prompt_number": 437
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from tsne import *\n",
      "#perm=np.random.permutation(np.size(Y_train))\n",
      "#X_train_red=np.array(X_train)[perm,:]\n",
      "#Y_train_red=np.array(Y_train)[perm]\n",
      "\n",
      "aux = np.concatenate((X_train,X_test),axis=0)\n",
      "#aux = X_test\n",
      "#aux = aux + 0.1*np.random.rand(np.shape(aux)[0],np.shape(aux)[1])\n",
      "Y = tsne(np.float64(aux), 2, 6, 20.0);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Preprocessing the data using PCA...\n",
        "Computing pairwise distances...\n",
        "Computing P-values for point  0  of  1045 ...\n",
        "Computing P-values for point "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500  of  1045 ...\n",
        "Computing P-values for point "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000  of  1045 ...\n",
        "Mean value of sigma: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.826156609787\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 : error is  19.6304917237\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20 : error is  16.5878516504\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 30 : error is  14.1476423751\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 40 : error is  13.0314045814\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50 : error is  12.636178853\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 60 : error is  12.45169432\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 70 : error is  12.339947381\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 80 : error is  12.249701452\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 90 : error is  12.1810078717\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 : error is  12.1352807269\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 110 : error is  1.40639184229\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 120 : error is  1.18873167883\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 130 : error is  1.01121336092\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 140 : error is  0.883488589927\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 : error is  0.79475043187\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 160 : error is  0.733477994479\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 170 : error is  0.689380839751\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 180 : error is  0.655765344428\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 190 : error is  0.629189069683\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200 : error is  0.607750984447\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 210 : error is  0.59061249542\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 220 : error is  0.576525454921\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 230 : error is  0.564707574667\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 240 : error is  0.554778508635\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250 : error is  0.546367793978\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 260 : error is  0.539148614285\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 270 : error is  0.532837508729\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 280 : error is  0.527264483223\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 290 : error is  0.522342292526\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300 : error is  0.518038861005\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 310 : error is  0.514154786679\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 320 : error is  0.510635000324\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 330 : error is  0.507521419262\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 340 : error is  0.504688572903\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 350 : error is  0.502099696479\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 360 : error is  0.499738998727\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 370 : error is  0.497560778839\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 380 : error is  0.495543298569\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 390 : error is  0.493680898347\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 400 : error is  0.491951159035\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 410 : error is  0.490346530902\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 420 : error is  0.488849195242\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 430 : error is  0.487462496009\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 440 : error is  0.486165288423\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 450 : error is  0.484941479229\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 460 : error is  0.483776663015\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 470 : error is  0.48267476909\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 480 : error is  0.481635352817\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 490 : error is  0.480662007729\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 : error is  0.479746177735\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 510 : error is  0.478863718191\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 520 : error is  0.478016537232\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 530 : error is  0.477213184694\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 540 : error is  0.476449611278\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 550 : error is  0.475723252195\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 560 : error is  0.475033637689\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 570 : error is  0.474364997497\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 580 : error is  0.473708766603\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 590 : error is  0.473087239618\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 600 : error is  0.47250539907\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 610 : error is  0.471949428923\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 620 : error is  0.471407227021\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 630 : error is  0.470875822554\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 640 : error is  0.470365074127\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 650 : error is  0.469876298958\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 660 : error is  0.469403817988\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 670 : error is  0.468948008311\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 680 : error is  0.468510048105\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 690 : error is  0.468082538371\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 700 : error is  0.467663717713\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 710 : error is  0.467262549156\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 720 : error is  0.466877953319\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 730 : error is  0.466504703784\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 740 : error is  0.466140649922\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 750 : error is  0.465785599115\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 760 : error is  0.465438983505\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 770 : error is  0.465107971865\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 780 : error is  0.464784074295\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 790 : error is  0.464465575297\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 800 : error is  0.464158471202\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 810 : error is  0.463861270639\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 820 : error is  0.463570495246\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 830 : error is  0.46328514487\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 840 : error is  0.463006238375\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 850 : error is  0.462736337482\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 860 : error is  0.462473914893\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 870 : error is  0.462216638889\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 880 : error is  0.461966830693\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 890 : error is  0.461724140562\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 900 : error is  0.461486066549\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 910 : error is  0.461251587955\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 920 : error is  0.461024295058\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 930 : error is  0.46080049641\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 940 : error is  0.46057817019\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 950 : error is  0.460362512538\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 960 : error is  0.460155511623\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 970 : error is  0.459954111766\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 980 : error is  0.459755024786\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 990 : error is  0.459558364377\n",
        "Iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 : error is  0.459368067547\n"
       ]
      }
     ],
     "prompt_number": 430
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Error=-np.log([np.array(Y_pred_Tot)[i,Y_train[i]] for i in range(np.shape(Y_pred_Tot)[0])])\n",
      "Error2=-np.log([np.array(Yt_pred)[i,Y_test[i]] for i in range(np.shape(Yt_pred)[0])])\n",
      "ErrorT = np.concatenate((np.atleast_2d(Error),np.atleast_2d(Error2)),axis=1)\n",
      "\n",
      "x = np.array([\"black\",\"black\"])\n",
      "RSet = np.repeat(x, [0, np.size(Error)], axis=0)\n",
      "x = np.array([\"red\",\"red\"])\n",
      "TSet = np.repeat(x, [0, np.size(Error2)], axis=0)\n",
      "Set = np.concatenate((np.atleast_2d(RSet),np.atleast_2d(TSet)),axis=1)\n",
      "import plotly.plotly as py\n",
      "#Error_red = Error[perm]\n",
      "#Error = error=np.random.rand(1,1000)\n",
      "\n",
      "labels = np.concatenate((Y_train,Y_test),axis=0)\n",
      "#labels = Y_test\n",
      "fig, ax = fig, ax = plt.subplots()\n",
      "plt.scatter(Y[:,0], Y[:,1], s=np.maximum(500*ErrorT,10), c=labels,  alpha=0.8,cmap=plt.cm.Set1, edgecolor=Set[0]);\n",
      "#plt.legend(label='a')\n",
      "py.iplot_mpl(fig)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~hugoguh/120.embed\" height=\"525\" width=\"100%\"></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 431,
       "text": [
        "<plotly.tools.PlotlyDisplay at 0x1106bdbd0>"
       ]
      }
     ],
     "prompt_number": 431
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Set[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 426,
       "text": [
        "array(['red', 'red', 'red', ..., 'black', 'black', 'black'], \n",
        "      dtype='|S5')"
       ]
      }
     ],
     "prompt_number": 426
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = fig, ax = plt.subplots()\n",
      "plt.scatter(np.array([1,2,3,4,5]), np.array([1,2,3,4,5]), s=200, c=np.array([1,2,3,4,5]), alpha=0.8,cmap=plt.cm.Set1);\n",
      "#plt.legend()\n",
      "py.iplot_mpl(fig)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~hugoguh/119.embed\" height=\"525\" width=\"100%\"></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 429,
       "text": [
        "<plotly.tools.PlotlyDisplay at 0x10e5fe690>"
       ]
      }
     ],
     "prompt_number": 429
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}